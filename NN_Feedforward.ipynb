{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "backed-crest",
   "metadata": {},
   "source": [
    "Feedfroward propagation is the first step for computing the output and letting us know the error. After first pass of feedword we are going to compute backpropagation for optimizing the weights. Lets go through the example and understand it completely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-shanghai",
   "metadata": {},
   "source": [
    "For this tutorial, we’re going to use a neural network with two inputs, two hidden neurons, two output neurons. Additionally, the hidden and output neurons will include a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-writing",
   "metadata": {},
   "source": [
    "Here’s the basic structure:\n",
    "\n",
    "![NN](images/NN/NN_1.png)\n",
    "\n",
    "In order to have some numbers to work with, here are the **initial weights**, the **biases**, and **training inputs/outputs**\n",
    "\n",
    "![](images/NN/NN_1_numbers.png)\n",
    "\n",
    "The goal of backpropagation is to optimize the weights so that the neural network can learn how to correctly map arbitrary inputs to outputs.\n",
    "\n",
    "For the rest of this tutorial we’re going to work with a single training set: given inputs 0.05 and 0.10, we want the neural network to output 0.01 and 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-ebony",
   "metadata": {},
   "source": [
    "### The Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-livestock",
   "metadata": {},
   "source": [
    "To begin, lets see what the neural network currently predicts given the weights and biases above and inputs of 0.05 and 0.10. To do this we’ll feed those inputs forward though the network.\n",
    "\n",
    "We figure out the total net input to each hidden layer neuron, squash the total net input using an activation function (here we use the logistic function), then repeat the process with the output layer neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-unknown",
   "metadata": {},
   "source": [
    "Total net input is also referred to as just net input by [some sources](https://www.cs.swarthmore.edu/~meeden/cs81/s10/BackPropDeriv.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-accreditation",
   "metadata": {},
   "source": [
    "Here’s how we calculate the total net input for $h_1$:\n",
    "\n",
    "$$ net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1 $$\n",
    "$$ net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1  = 0.3775$$\n",
    "\n",
    "We then squash it using the logistic function to get the output of $h_1$:\n",
    "$$ out_{h1} = \\frac{1}{1+e^{-net_{h1}}} =  \\frac{1}{1+e^{-0.3773}} = 0.593269992 $$\n",
    "\n",
    "Carrying out the same process for $h_2$ we get: \n",
    "$$ out_{h2} = 0.596884378 $$\n",
    "\n",
    "We repeat this process for the output layer neurons, using the output from the hidden layer neurons as inputs.\n",
    "\n",
    "Here’s the output for $o_1$:\n",
    "$$ net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1 $$\n",
    "$$ net_{01} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1  = 1.105905967 $$\n",
    "$$ out_{01} = \\frac{1}{1+e^{-1.105905967}} = 0.75136507 $$\n",
    "And carrying out the same process for $o_2$ we get:\n",
    "$$ out_{o2} = 0.772928465 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-somalia",
   "metadata": {},
   "source": [
    "**Calculating the Total Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-humanitarian",
   "metadata": {},
   "source": [
    "We can now calculate the error for each output neuron using the squared error function and sum them to get the total error\n",
    "\n",
    "$$ E_{total} = \\sum \\frac{1}{2} (target-output)^2 $$\n",
    "\n",
    "The $\\frac{1}{2}$ `is included so that exponent is cancelled when we differentiate later on. The result is eventually multiplied by a learning rate anyway so it doesn’t matter that we introduce a constant here\n",
    "`\n",
    "\n",
    "For example, the target output for $o_1$ is 0.01 but the neural network output 0.75136507, therefore its error is:\n",
    "$$ E_{o1} = \\frac{1}{2} (target-out_{o1})^2 = \\frac{1}{2} (0.01 - 0.75136507))^2 = 0.274811083 $$\n",
    "\n",
    "Repeating this process for $o_2$ (remembering that the target is 0.99) we get:\n",
    "$$ E_{o2} = 0.023560026 $$\n",
    "\n",
    "The total error for the neural network is the sum of these errors:\n",
    "$$ E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-champagne",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-direction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
